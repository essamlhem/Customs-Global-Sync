import requests
import random
import time
import re

class SupabaseScraper:
    def __init__(self):
        self.session = requests.Session()
        # Ù‚Ø§Ø¦Ù…Ø© Ù…ØªØµÙØ­Ø§Øª Ø­Ø¯ÙŠØ«Ø© Ø¬Ø¯Ø§Ù‹ Ù„ØªØ¨Ø¯Ùˆ ÙƒØ£Ù†Ù‡Ø§ Ø·Ù„Ø¨Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ©
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) Gecko/20100101 Firefox/122.0'
        ]

    def get_real_images(self, brand, model):
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØ­Ø°Ù Ø£ÙŠ Ù‚ÙŠÙ… ÙØ§Ø±ØºØ© Ø£Ùˆ ØºÙŠØ± Ù…ÙÙ‡ÙˆÙ…Ø©
        query = f"{brand} {model}".replace("nan", "").strip()
        if not query or len(query) < 3:
            return []

        print(f"ğŸ“¡ Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ù†Øµ ØµÙˆØ± Ù„Ù€: {query}")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø­Ø±Ùƒ Ø¨Ø­Ø« Ø¨Ø¯ÙŠÙ„ (Bing) Ø¨Ø£Ø³Ù„ÙˆØ¨ Ù…Ø¨Ø§Ø´Ø± Ø£Ùˆ DuckDuckGo Ø§Ù„Ù…Ø­Ø¯Ø«
        search_url = f"https://duckduckgo.com/i.js?q={query}&o=json&v=1&f=,,,&p=1"
        
        headers = {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Referer': 'https://duckduckgo.com/',
            'X-Requested-With': 'XMLHttpRequest'
        }

        try:
            # Ø¥Ø¶Ø§ÙØ© ØªØ£Ø®ÙŠØ± Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¨Ø´Ø±ÙŠ (Ø¨ÙŠÙ† 2 Ø¥Ù„Ù‰ 4 Ø«ÙˆØ§Ù†Ù)
            # Ù‡Ø°Ø§ Ø§Ù„Ø³Ø± ÙÙŠ Ù…Ù†Ø¹ Ø¸Ù‡ÙˆØ± Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ÙØ§Ø±ØºØ©
            time.sleep(random.uniform(2.0, 4.0))
            
            response = self.session.get(search_url, headers=headers, timeout=15)
            
            if response.status_code == 200:
                try:
                    data = response.json()
                    results = data.get('results', [])
                    
                    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ØµÙˆØ±
                    image_urls = []
                    for r in results:
                        img = r.get('image')
                        if img and any(img.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                            image_urls.append(img)
                    
                    if image_urls:
                        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(image_urls[:5])} ØµÙˆØ±.")
                        return image_urls[:5]
                    else:
                        print(f"âš ï¸ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø£Ø¹Ø§Ø¯ Ù†ØªØ§Ø¦Ø¬ ÙˆÙ„ÙƒÙ† Ø¨Ø¯ÙˆÙ† Ø±ÙˆØ§Ø¨Ø· ØµÙˆØ±.")
                except Exception as e:
                    print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ JSON: {e}")
            elif response.status_code == 403:
                print("ğŸš« Ø­Ø¸Ø± (403): Ø§Ù„Ù…ÙˆÙ‚Ø¹ ÙƒØ´Ù Ø§Ù„Ø³ÙƒØ±ÙŠØ¨ØªØŒ ÙŠØ­ØªØ§Ø¬ Ù„ØªØ£Ø®ÙŠØ± Ø£Ø·ÙˆÙ„.")
            else:
                print(f"ğŸ›‘ Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹Ø©: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ ÙØ´Ù„ Ø§Ù„Ø§ØªØµØ§Ù„: {e}")
        
        return []
