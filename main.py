import os
import pandas as pd
import json
import time
from Scraper import SupabaseScraper

CACHE_FILE = "images_cache.json"

def git_push_progress(count):
    """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø¯Ù… ÙˆØ±ÙØ¹Ù‡ Ø¥Ù„Ù‰ GitHub"""
    try:
        os.system('git config --local user.email "action@github.com"')
        os.system('git config --local user.name "GitHub Action"')
        os.system(f'git add {CACHE_FILE} Across_MENA_Final_Report.csv')
        os.system(f'git commit -m "ØªØ­Ø¯ÙŠØ« ØªÙ„Ù‚Ø§Ø¦ÙŠ: ØµÙŠØ¯ {count} Ù…Ù†ØªØ¬ Ø¬Ø¯ÙŠØ¯ Ø¨Ù€ 6 ØµÙˆØ±"')
        os.system('git push')
        print(f"â˜ï¸ [GitHub] ØªÙ… Ø±ÙØ¹ Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø±ÙØ¹ Ù„Ù€ GitHub: {e}")

def main():
    scraper = SupabaseScraper()
    csv_file = "data.csv"
    
    if not os.path.exists(csv_file):
        print("âŒ Ø§Ù„Ù…Ù„Ù data.csv ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return

    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ØµÙ„ÙŠØ©
    df_input = pd.read_csv(csv_file)
    raw_data = df_input.to_dict(orient='records')

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ§Ø´ Ø§Ù„Ø­Ø§Ù„ÙŠ Ø£Ùˆ Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ§Ø­Ø¯ Ø¬Ø¯ÙŠØ¯
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                image_cache = json.load(f)
        except:
            image_cache = {}
    else:
        image_cache = {}

    final_list = []
    updated_count = 0

    for index, item in enumerate(raw_data):
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„ÙƒÙ„ Ù…Ø§Ø¯Ø©
        item_id = str(item.get('id', index))
        
        # Ø§Ù„Ù‚ÙˆØ© Ù‡Ù†Ø§: Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© ÙØ§Ø±ØºØ© [] Ø£Ùˆ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ø³ÙŠØ¹ÙŠØ¯ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù†Ù‡Ø§
        # Ù„Ù† ÙŠØªØ®Ø·Ù‰ Ø§Ù„Ù…Ø§Ø¯Ø© Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙˆØ¬Ø¯ Ù‚Ø§Ø¦Ù…Ø© ØªØ­ØªÙˆÙŠ ÙØ¹Ù„ÙŠØ§Ù‹ Ø¹Ù„Ù‰ Ø±ÙˆØ§Ø¨Ø· ØµÙˆØ±
        existing_images = image_cache.get(item_id, [])
        if isinstance(existing_images, list) and len(existing_images) >= 6:
            item['image_urls'] = existing_images
            final_list.append(item)
            continue

        brand = str(item.get('brand', '')).replace('nan', '')
        model = str(item.get('model', '')).replace('nan', '')
        
        print(f"ğŸ“¡ Ø¬Ø§Ø±ÙŠ ØµÙŠØ¯ (6 ØµÙˆØ±) Ù„Ù€: {brand} {model} [{index+1}/{len(raw_data)}]")
        
        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ù„Ø¬Ù„Ø¨ 6 ØµÙˆØ±
        new_images = scraper.get_real_images(brand, model)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙƒØ§Ø´ ÙˆØ§Ù„Ù‚Ø§Ø¦Ù…Ø©
        image_cache[item_id] = new_images
        item['image_urls'] = new_images
        final_list.append(item)
        updated_count += 1

        # Ø±ÙØ¹ Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ 30 Ø¹Ù…Ù„ÙŠØ© ØµÙŠØ¯ Ù†Ø§Ø¬Ø­Ø© Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø¶ÙŠØ§Ø¹ Ø§Ù„ØªØ¹Ø¨
        if updated_count > 0 and updated_count % 30 == 0:
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(image_cache, f, ensure_ascii=False, indent=4)
            pd.DataFrame(final_list).to_csv("Across_MENA_Final_Report.csv", index=False, encoding='utf-8-sig')
            git_push_progress(updated_count)

    # Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ø´Ø§Ù…Ù„
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(image_cache, f, ensure_ascii=False, indent=4)
    pd.DataFrame(final_list).to_csv("Across_MENA_Final_Report.csv", index=False, encoding='utf-8-sig')
    git_push_progress("Ø§Ù„Ù…Ù‡Ù…Ø© ÙƒØ§Ù…Ù„Ø©")
    print("ğŸ Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ØµÙŠØ¯ Ø¨Ù†Ø¬Ø§Ø­!")

if __name__ == "__main__":
    main()
