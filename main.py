import os
import pandas as pd
import json
import time
from Scraper import SupabaseScraper

CACHE_FILE = "images_cache.json"

def git_push_progress(count):
    """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø¯Ù… ÙˆØ±ÙØ¹Ù‡ Ø¥Ù„Ù‰ GitHub"""
    try:
        os.system('git config --local user.email "action@github.com"')
        os.system('git config --local user.name "GitHub Action"')
        os.system(f'git add {CACHE_FILE} Across_MENA_Final_Report.csv')
        os.system(f'git commit -m "ØªØ­Ø¯ÙŠØ«: ØªÙ… ØµÙŠØ¯ {count} ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©"')
        os.system('git push')
        print(f"â˜ï¸ [GitHub] ØªÙ… Ø±ÙØ¹ Ø§Ù„ØªÙ‚Ø¯Ù… Ø¨Ù†Ø¬Ø§Ø­!")
    except Exception as e:
        print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø±ÙØ¹: {e}")

def main():
    # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯
    scraper = SupabaseScraper()
    csv_file = "data.csv"
    
    if not os.path.exists(csv_file):
        print("âŒ Ù…Ù„Ù data.csv ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return

    # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
    df_input = pd.read_csv(csv_file)
    raw_data = df_input.to_dict(orient='records')

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ§Ø´ (Ù„Ùˆ Ù…ÙˆØ¬ÙˆØ¯)
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                image_cache = json.load(f)
        except:
            image_cache = {}
    else:
        image_cache = {}

    final_list = []
    new_items_processed = 0

    for index, item in enumerate(raw_data):
        # Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù€ ID Ø£Ùˆ Ø±Ù‚Ù… Ø§Ù„Ø³Ø·Ø± ÙƒÙ…Ø±Ø¬Ø¹
        item_id = str(item.get('id', index))
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù†ØªØ¬ Ù…Ø³Ø¬Ù„ Ø³Ø§Ø¨Ù‚Ø§Ù‹ ÙˆÙÙŠÙ‡ ØµÙˆØ± (Ø£ÙƒØ«Ø± Ù…Ù† 0)ØŒ Ù†ØªØ®Ø·Ø§Ù‡
        if item_id in image_cache and isinstance(image_cache[item_id], list) and len(image_cache[item_id]) > 0:
            item['image_urls'] = image_cache[item_id]
            final_list.append(item)
            continue

        brand = str(item.get('brand', '')).replace('nan', '')
        model = str(item.get('model', '')).replace('nan', '')
        
        print(f"ğŸ” [{index+1}/{len(raw_data)}] Ø¬Ø§Ø±ÙŠ ØµÙŠØ¯ ØµÙˆØ± Ù„Ù€: {brand} {model}")
        
        # Ø·Ù„Ø¨ Ø§Ù„Ù€ 6 ØµÙˆØ± Ù…Ù† Ø§Ù„Ø³ÙƒØ±Ø§Ø¨Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯
        found_images = scraper.get_real_images(brand, model)
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ø§Ù„ÙƒØ§Ø´ ÙˆÙÙŠ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        image_cache[item_id] = found_images
        item['image_urls'] = found_images
        final_list.append(item)
        
        new_items_processed += 1

        # Ø±ÙØ¹ Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ 30 Ù…Ù†ØªØ¬ (Ù‚Ù„Ù„Øª Ø§Ù„Ø¹Ø¯Ø¯ Ø¹Ø´Ø§Ù† ØªØ¶Ù…Ù† Ø§Ù„Ø­ÙØ¸ Ø£Ø³Ø±Ø¹)
        if new_items_processed % 30 == 0:
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(image_cache, f, ensure_ascii=False, indent=4)
            
            pd.DataFrame(final_list).to_csv("Across_MENA_Final_Report.csv", index=False, encoding='utf-8-sig')
            git_push_progress(new_items_processed)
            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ {new_items_processed} Ù…Ø§Ø¯Ø© Ø¨Ù†Ø¬Ø§Ø­...")

    # Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø¹Ù†Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¯ÙˆØ±Ø©
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(image_cache, f, ensure_ascii=False, indent=4)
    pd.DataFrame(final_list).to_csv("Across_MENA_Final_Report.csv", index=False, encoding='utf-8-sig')
    git_push_progress("Ø§Ù„ÙƒÙ„")
    print("ğŸ Ø§ÙƒØªÙ…Ù„Øª Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ù†Ø¬Ø§Ø­!")

if __name__ == "__main__":
    main()
