import os
import requests
import pandas as pd
import json
import time
from datetime import datetime
from Scraper import SupabaseScraper

CACHE_FILE = "images_cache.json"

def main():
    print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„: {datetime.now()}")
    scraper = SupabaseScraper()
    csv_file = "data.csv"
    
    # 1. Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ù„ÙŠØ©
    if not os.path.exists(csv_file):
        print("âŒ Ù…Ù„Ù data.csv ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return
    
    try:
        df_input = pd.read_csv(csv_file)
        raw_data = df_input.to_dict(orient='records')
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(raw_data)} Ù…Ø§Ø¯Ø©.")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© CSV: {e}")
        return

    # 2. ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ§Ø´ (Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ù†Ø¬Ø²Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹)
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            image_cache = json.load(f)
        print(f"ğŸ“¦ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒØ§Ø´ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(image_cache)} Ù…Ø§Ø¯Ø©.")
    else:
        image_cache = {}

    final_list = []
    new_images_count = 0
    total_items = len(raw_data)

    # 3. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø®Ø§ØµÙŠØ© Ø§Ù„Ø§Ø³ØªÙƒÙ…Ø§Ù„ (Resume)
    for index, item in enumerate(raw_data):
        # Ù…Ø¹Ø±Ù ÙØ±ÙŠØ¯ Ù„ÙƒÙ„ Ù…Ø§Ø¯Ø©
        item_id = str(item.get('id', f"{item.get('brand')}_{item.get('model')}"))
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø§Ø¯Ø© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„ÙƒØ§Ø´ ÙˆØ¨Ù‡Ø§ ØµÙˆØ±ØŒ Ù†ØªØ®Ø·Ø§Ù‡Ø§
        if item_id in image_cache and image_cache[item_id]:
            item['image_urls'] = image_cache[item_id]
            final_list.append(item)
            continue
        
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©ØŒ Ù†Ø¨Ø­Ø« Ø¹Ù† ØµÙˆØ±Ù‡Ø§
        brand = str(item.get('brand', ''))
        model = str(item.get('model', ''))
        print(f"ğŸ” [{index+1}/{total_items}] Ø³Ø­Ø¨ ØµÙˆØ± Ù„Ù€: {brand} {model}")
        
        imgs = scraper.get_real_images(brand, model)
        image_cache[item_id] = imgs
        item['image_urls'] = imgs
        new_images_count += 1
        final_list.append(item)

        # Ø­ÙØ¸ Ø§Ù„ÙƒØ§Ø´ ÙƒÙ„ 50 ØµÙˆØ±Ø© Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø¶ÙŠØ§Ø¹ Ø§Ù„Ø¬Ù‡Ø¯ Ø¥Ø°Ø§ ÙØµÙ„ Ø§Ù„Ø³ÙŠØ±ÙØ±
        if new_images_count % 50 == 0:
            with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                json.dump(image_cache, f, ensure_ascii=False, indent=4)
            print(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø¯Ù… Ù…Ø¤Ù‚Øª ({new_images_count} ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©)")
            time.sleep(1) # ØªØ£Ø®ÙŠØ± Ø¨Ø³ÙŠØ· Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø­Ø¸Ø±

    # 4. Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(image_cache, f, ensure_ascii=False, indent=4)

    df_final = pd.DataFrame(final_list)
    output_file = "Across_MENA_Final_Report.csv"
    df_final.to_csv(output_file, index=False, encoding='utf-8-sig')

    # 5. Ø¥Ø±Ø³Ø§Ù„ Ø¥Ø´Ø¹Ø§Ø± ØªÙ„ÙŠØ¬Ø±Ø§Ù…
    bot_token = os.getenv("BOT_TOKEN")
    chat_id = os.getenv("CHAT_ID")
    if bot_token and chat_id:
        done_count = len(image_cache)
        msg = (f"â³ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ø¯ÙˆØ±ÙŠ:\n"
               f"âœ… ØªÙ… Ø¥Ù†Ø¬Ø§Ø²: {done_count} Ù…Ù† Ø£ØµÙ„ {total_items}\n"
               f"ğŸ“¸ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø© ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¯ÙˆØ±Ø©: {new_images_count}")
        try:
            requests.post(f"https://api.telegram.org/bot{bot_token}/sendMessage", data={'chat_id': chat_id, 'text': msg})
        except: pass

    print(f"ğŸ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¯ÙˆØ±Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©. Ø§Ù„Ù…Ù†Ø¬Ø² Ø§Ù„ÙƒÙ„ÙŠ: {len(image_cache)}")

if __name__ == "__main__":
    main()
