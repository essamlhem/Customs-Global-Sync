import os
import requests
import pandas as pd
import json
import time
from datetime import datetime
from Scraper import SupabaseScraper

CACHE_FILE = "images_cache.json"

def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f: return json.load(f)
        except: return {}
    return {}

def save_cache(cache):
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(cache, f, ensure_ascii=False, indent=4)

def main():
    print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„Ø°ÙƒÙŠ: {datetime.now()}")
    scraper = SupabaseScraper()
    image_cache = load_cache()
    
    all_data = []
    offset = 0
    limit = 500 # Ù†Ø³Ø­Ø¨ 500 Ù…Ø§Ø¯Ø© ÙÙŠ ÙƒÙ„ Ø·Ù„Ø¨ Ù„Ø³ÙˆØ¨Ø§Ø¨ÙŠØ³
    
    # 1. Ø¬Ù„Ø¨ ÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø³ÙˆØ¨Ø§Ø¨ÙŠØ³ Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª
    print("ğŸ“¥ Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Supabase...")
    while True:
        batch = scraper.fetch_raw_data_batched(offset=offset, limit=limit)
        if not batch: break
        all_data.extend(batch)
        offset += limit
        if len(batch) < limit: break # Ø¥Ø°Ø§ Ø±Ø¬Ø¹ Ø£Ù‚Ù„ Ù…Ù† 500 ÙŠØ¹Ù†ÙŠ ÙˆØµÙ„Ù†Ø§ Ù„Ù„Ù†Ù‡Ø§ÙŠØ©
        time.sleep(0.5) # Ø±Ø§Ø­Ø© Ù„Ù„Ø³ÙŠØ±ÙØ±

    print(f"âœ… ØªÙ… Ø¬Ù„Ø¨ {len(all_data)} Ù…Ø§Ø¯Ø©. Ù†Ø¨Ø¯Ø£ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ±...")

    final_list = []
    new_images = 0

    # 2. Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØ± (Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ø´)
    for item in all_data:
        item_id = str(item.get('id', item.get('hs_code', '')))
        
        if item_id in image_cache and len(image_cache[item_id]) > 0:
            imgs = image_cache[item_id]
        else:
            # Ù†Ø³Ø­Ø¨ Ø§Ù„ØµÙˆØ± ÙÙ‚Ø· Ø¥Ø°Ø§ Ù…Ùˆ Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¨Ø§Ù„ÙƒØ§Ø´
            imgs = scraper.get_real_images(item.get('brand', ''), item.get('model', ''))
            image_cache[item_id] = imgs
            new_images += 1
            if new_images % 10 == 0:
                save_cache(image_cache) # Ø­ÙØ¸ Ø¯ÙˆØ±ÙŠ Ø¹Ø´Ø§Ù† Ù„Ùˆ ÙØµÙ„ Ù…Ø§ Ù†Ø¶ÙŠØ¹ Ø´ÙŠ
                time.sleep(1)

        item['image_urls'] = imgs # Ø§Ù„ØªØ³Ù…ÙŠØ© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù„ÙŠ Ø·Ù„Ø¨Ù‡Ø§ Ø§Ù„Ù…Ø¨Ø±Ù…Ø¬
        final_list.append(item)

    # 3. ØªÙ†Ø¸ÙŠÙ ÙˆØ­ÙØ¸ Ø§Ù„Ù…Ù„Ù
    df = pd.DataFrame(final_list)
    to_drop = ['material', 'note', 'band-material', 'band_material']
    df_final = df.drop(columns=[c for c in to_drop if c in df.columns])
    
    file_name = "Across_MENA_Report.csv"
    df_final.to_csv(file_name, index=False, encoding='utf-8-sig')

    # 4. Ø¥Ø±Ø³Ø§Ù„ Ù„ØªÙ„ÙŠØ¬Ø±Ø§Ù…
    bot_token = os.getenv("BOT_TOKEN")
    chat_id = os.getenv("CHAT_ID")
    msg = f"âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ø¯ÙŠØ«!\nğŸ”¹ Ø§Ù„Ù…ÙˆØ§Ø¯: {len(df_final)}\nğŸ”¹ ØµÙˆØ± Ø¬Ø¯ÙŠØ¯Ø©: {new_images}"
    
    requests.post(f"https://api.telegram.org/bot{bot_token}/sendMessage", data={'chat_id': chat_id, 'text': msg})
    with open(file_name, 'rb') as f:
        requests.post(f"https://api.telegram.org/bot{bot_token}/sendDocument", data={'chat_id': chat_id}, files={'document': f})

if __name__ == "__main__":
    main()
