import os
import requests
import pandas as pd
import json
import time
from datetime import datetime
from Scraper import SupabaseScraper

CACHE_FILE = "images_cache.json"

def git_push_progress(count):
    """ÙˆØ¸ÙŠÙØ© Ù„Ø±ÙØ¹ Ø§Ù„ØªÙ‚Ø¯Ù… Ù„Ù€ GitHub ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
    try:
        os.system('git config --local user.email "action@github.com"')
        os.system('git config --local user.name "GitHub Action"')
        os.system(f'git add {CACHE_FILE} data.csv Across_MENA_Final_Report.csv')
        os.system(f'git commit -m "Ø­ÙØ¸ ØªÙ„Ù‚Ø§Ø¦ÙŠ: ØªÙ… Ø¥Ù†Ø¬Ø§Ø² {count} ØµÙˆØ±Ø© Ø¬Ø¯ÙŠØ¯Ø©"')
        os.system('git push')
        print(f"â˜ï¸ [GitHub] ØªÙ… Ø±ÙØ¹ Ø§Ù„ØªÙ‚Ø¯Ù… Ø¨Ù†Ø¬Ø§Ø­ (Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙˆØ± Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©: {count})")
    except Exception as e:
        print(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø±ÙØ¹ Ù„Ù€ GitHub: {e}")

def main():
    print(f"ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ´ØºÙŠÙ„: {datetime.now()}")
    scraper = SupabaseScraper()
    csv_file = "data.csv"
    
    if not os.path.exists(csv_file):
        print("âŒ Ù…Ù„Ù data.csv ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
        return
    
    try:
        df_input = pd.read_csv(csv_file)
        raw_data = df_input.to_dict(orient='records')
        print(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ {len(raw_data)} Ù…Ø§Ø¯Ø©.")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© CSV: {e}")
        return

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒØ§Ø´ (Ø§Ù„Ù…ÙˆØ§Ø¯ Ø§Ù„Ù…Ù†Ø¬Ø²Ø© Ø³Ø§Ø¨Ù‚Ø§Ù‹)
    if os.path.exists(CACHE_FILE):
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            image_cache = json.load(f)
        print(f"ğŸ“¦ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒØ§Ø´ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(image_cache)} Ù…Ø§Ø¯Ø©.")
    else:
        image_cache = {}

    final_list = []
    new_images_count = 0
    total_items = len(raw_data)

    for index, item in enumerate(raw_data):
        item_id = str(item.get('id', f"{item.get('brand')}_{item.get('model')}"))
        
        # ØªØ®Ø·ÙŠ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹ Ù…Ø³Ø¨Ù‚Ø§Ù‹
        if item_id in image_cache and image_cache[item_id]:
            item['image_urls'] = image_cache[item_id]
            final_list.append(item)
            continue
        
        brand = str(item.get('brand', ''))
        model = str(item.get('model', ''))
        print(f"ğŸ” [{index+1}/{total_items}] Ø³Ø­Ø¨ ØµÙˆØ± Ù„Ù€: {brand} {model}")
        
        try:
            imgs = scraper.get_real_images(brand, model)
            image_cache[item_id] = imgs
            item['image_urls'] = imgs
            new_images_count += 1
            final_list.append(item)

            # Ø§Ù„Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ù„ÙŠ ÙˆØ§Ù„Ø±ÙØ¹ Ù„Ù€ GitHub ÙƒÙ„ 50 ØµÙˆØ±Ø©
            if new_images_count > 0 and new_images_count % 50 == 0:
                with open(CACHE_FILE, 'w', encoding='utf-8') as f:
                    json.dump(image_cache, f, ensure_ascii=False, indent=4)
                
                # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ø¤Ù‚Øª Ù„Ø¶Ù…Ø§Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
                pd.DataFrame(final_list).to_csv("Across_MENA_Final_Report.csv", index=False, encoding='utf-8-sig')
                
                print(f"ğŸ’¾ Ø­ÙØ¸ Ù…Ø¤Ù‚Øª Ù…Ø­Ù„ÙŠ Ù„Ù€ {new_images_count} ØµÙˆØ±Ø©.")
                git_push_progress(new_images_count) # Ø§Ù„Ø±ÙØ¹ Ù„Ù€ GitHub
                time.sleep(2) # ØªØ£Ø®ÙŠØ± Ù„ØªØ¬Ù†Ø¨ Ø£ÙŠ ØªØ¹Ù„ÙŠÙ‚ ÙÙŠ Ø§Ù„Ù€ Push

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø§Ø¯Ø© {item_id}: {e}")
            continue

    # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø¹Ù†Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„Ø¯ÙˆØ±Ø©
    with open(CACHE_FILE, 'w', encoding='utf-8') as f:
        json.dump(image_cache, f, ensure_ascii=False, indent=4)

    df_final = pd.DataFrame(final_list)
    df_final.to_csv("Across_MENA_Final_Report.csv", index=False, encoding='utf-8-sig')
    git_push_progress(new_images_count)

    print(f"ğŸ Ø§Ù†ØªÙ‡Øª Ø§Ù„Ø¯ÙˆØ±Ø©. Ø§Ù„Ù…Ù†Ø¬Ø² Ø§Ù„ÙƒÙ„ÙŠ: {len(image_cache)}")

if __name__ == "__main__":
    main()
